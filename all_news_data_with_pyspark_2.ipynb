{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all news data with pyspark 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4gJ047dxirg"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFIjxWfmxipH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an9v3Q-Nxil0"
      },
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE2IRxyRxikP"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veTexPbExiV9"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU8GLMfZx7pr"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, lower, regexp_replace,concat,lit,split\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from pyspark.sql.functions import udf\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from itertools import filterfalse\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z80I2TDQyFe9",
        "outputId": "b50f20ff-0113-471e-ea9a-f4fdc5ca701f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0uToISLyNdy"
      },
      "source": [
        "!wget -q !wget  https://www.dropbox.com/s/cn2utnr5ipathhh/all-the-news-2-1.zip?dl=0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPDD36GoyNZC",
        "outputId": "48d6105c-ee01-45c6-ffb1-e505faf42a7c"
      },
      "source": [
        "! unzip /content/all-the-news-2-1.zip?dl=0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/all-the-news-2-1.zip?dl=0\n",
            "replace all-the-news-2-1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M745n_WnyJhK"
      },
      "source": [
        "data= spark.read.csv(\"/content/all-the-news-2-1.csv\",header=True,inferSchema=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvMMKlaGyMl7"
      },
      "source": [
        "new_data=data.select(data.columns[7:])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxVq61JB2hXQ"
      },
      "source": [
        "from pyspark.sql.functions import col,split,udf"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt3ZW4Km2j2F"
      },
      "source": [
        "final_data=new_data.drop(col(\"url\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJA9izSu2mEi",
        "outputId": "2ae25652-1167-4de8-973f-449e018ffbe4"
      },
      "source": [
        "final_data.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|               title|             article|             section|         publication|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|We should take co...|\"This post is par...| several critics ...|         for example|\n",
            "|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|\n",
            "|                null|                null|                null|                null|\n",
            "|Trump denies repo...|DAVOS, Switzerlan...|               Davos|             Reuters|\n",
            "|France's Sarkozy ...|PARIS (Reuters) -...|          World News|             Reuters|\n",
            "|Paris Hilton: Wom...|\"Paris Hilton arr...|                null|                 TMZ|\n",
            "|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|\n",
            "|                null|                null|                null|                null|\n",
            "|Venezuela detains...|CARACAS (Reuters)...|          World News|             Reuters|\n",
            "|You Can Trick You...|\"If only every da...| paying attention...| it makes it easi...|\n",
            "|How to watch the ...|Google I/O, the c...|                null|                 Vox|\n",
            "|China is dismissi...|China is dismissi...|                null|           Vice News|\n",
            "|“Elizabeth Warren...|Elizabeth Warren ...|                null|                 Vox|\n",
            "|Hudson's Bay's ch...|(Reuters) - The s...|       Business News|             Reuters|\n",
            "|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|                null|                 TMZ|\n",
            "|Jermaine Jackson ...|\"Jermaine Jackson...| and the 2 will f...| Quincy isn't say...|\n",
            "|UK PM May presses...|LONDON (Reuters) ...|          World News|             Reuters|\n",
            "|Nancy Pelosi says...|\"Nancy Pelosi is ...|           of course| but it begins lo...|\n",
            "|The government of...|The nonpartisan d...|                null|                 Vox|\n",
            "|Mark Zuckerberg’s...|The threat of gov...|                null|                 Vox|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3cMiM0f29aj"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HggSBaCf3B3B"
      },
      "source": [
        "from pyspark.sql.functions import lower,col,regexp_extract,filter,explode"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jErboO93FW1"
      },
      "source": [
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMne8JBPa9s1",
        "outputId": "204ed0fe-237b-40f7-d65a-e36680820317"
      },
      "source": [
        "def clean_text(c): ##cleaning text using clean_text function\n",
        "  c = lower(c)\n",
        "  c = regexp_replace(c, \"^rt \", \"\")\n",
        "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
        "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
        "  return c\n",
        "\n",
        "clean_text_df = final_data.select(clean_text(col(\"article\")).alias(\"text\"))\n",
        "\n",
        "clean_text_df.printSchema()\n",
        "clean_text_df.show(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            "\n",
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|this post is part...|\n",
            "| the indianapolis...|\n",
            "|                null|\n",
            "|davos switzerland...|\n",
            "|paris reuters  fo...|\n",
            "|paris hilton arri...|\n",
            "|berlin june 17 re...|\n",
            "|                null|\n",
            "|caracas reuters  ...|\n",
            "|if only every day...|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfJILAKm3KS5"
      },
      "source": [
        "@udf(returnType=T.ArrayType(T.StringType())) #create UDF function for tokenizeing data\n",
        "def word_tokenize(x):\n",
        "  if x == None:\n",
        "    return \"\"\n",
        "  else:\n",
        "    return nltk.word_tokenize(x)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gegd4Xb_3M50"
      },
      "source": [
        "tokenize_data=clean_text_df.select(word_tokenize(F.col(\"text\")).alias(\"text\"))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQRdZyVu3Oz1",
        "outputId": "6621a4fb-0f26-4fe6-e2fe-eac7db336e97"
      },
      "source": [
        "tokenize_data.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|[this, post, is, ...|\n",
            "|[the, indianapoli...|\n",
            "|                null|\n",
            "|[davos, switzerla...|\n",
            "|[paris, reuters, ...|\n",
            "|[paris, hilton, a...|\n",
            "|[berlin, june, 17...|\n",
            "|                null|\n",
            "|[caracas, reuters...|\n",
            "|[if, only, every,...|\n",
            "|[google, io, the,...|\n",
            "|[china, is, dismi...|\n",
            "|[elizabeth, warre...|\n",
            "|[reuters, the, su...|\n",
            "|[joakim, noahs, m...|\n",
            "|[jermaine, jackso...|\n",
            "|[london, reuters,...|\n",
            "|[nancy, pelosi, i...|\n",
            "|[the, nonpartisan...|\n",
            "|[the, threat, of,...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZOWzRJ3U2f"
      },
      "source": [
        "tokens = tokenize_data.select(explode(col(\"text\")).alias(\"tokens\"))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49u0_lD7331v",
        "outputId": "179a54e1-56d6-47fb-d4be-2128009bd2fe"
      },
      "source": [
        "tokens.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|     tokens|\n",
            "+-----------+\n",
            "|       this|\n",
            "|       post|\n",
            "|         is|\n",
            "|       part|\n",
            "|         of|\n",
            "|  polyarchy|\n",
            "|         an|\n",
            "|independent|\n",
            "|       blog|\n",
            "|   produced|\n",
            "|         by|\n",
            "|        the|\n",
            "|  political|\n",
            "|     reform|\n",
            "|    program|\n",
            "|         at|\n",
            "|        new|\n",
            "|    america|\n",
            "|          a|\n",
            "| washington|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITr82JT06ENM",
        "outputId": "80b12733-2edd-47a2-cd3f-42c9666fcc7a"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"tokens\", outputCol=\"vector\")\n",
        "vector_df = tokenizer.transform(tokens).select(\"vector\")\n",
        "\n",
        "vector_df.printSchema()\n",
        "vector_df.show(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- vector: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------------+\n",
            "|       vector|\n",
            "+-------------+\n",
            "|       [this]|\n",
            "|       [post]|\n",
            "|         [is]|\n",
            "|       [part]|\n",
            "|         [of]|\n",
            "|  [polyarchy]|\n",
            "|         [an]|\n",
            "|[independent]|\n",
            "|       [blog]|\n",
            "|   [produced]|\n",
            "+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T_676lK6iQu",
        "outputId": "e43200cf-738b-404a-d9de-f6f01c62d171"
      },
      "source": [
        "remover = StopWordsRemover()\n",
        "stopwords = remover.getStopWords() \n",
        "\n",
        "# Display default list\n",
        "stopwords[:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKsiOPDS6spZ",
        "outputId": "3ed982ca-9e57-46d2-a446-724ae3872468"
      },
      "source": [
        "# Specify input/output columns\n",
        "remover.setInputCol(\"vector\")\n",
        "remover.setOutputCol(\"vector_no_stopw\")\n",
        "\n",
        "# Transform existing dataframe with the StopWordsRemover\n",
        "vector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\")\n",
        "\n",
        "# Display\n",
        "vector_no_stopw_df.printSchema()\n",
        "vector_no_stopw_df.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- vector_no_stopw: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------------+\n",
            "|vector_no_stopw|\n",
            "+---------------+\n",
            "|             []|\n",
            "|         [post]|\n",
            "|             []|\n",
            "|         [part]|\n",
            "|             []|\n",
            "|    [polyarchy]|\n",
            "|             []|\n",
            "|  [independent]|\n",
            "|         [blog]|\n",
            "|     [produced]|\n",
            "|             []|\n",
            "|             []|\n",
            "|    [political]|\n",
            "|       [reform]|\n",
            "|      [program]|\n",
            "|             []|\n",
            "|          [new]|\n",
            "|      [america]|\n",
            "|             []|\n",
            "|   [washington]|\n",
            "+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJIcL37b_q0O"
      },
      "source": [
        "# Import stemmer library\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD-KZamQ_qvP"
      },
      "source": [
        "# Create stemmer python function\n",
        "def stem(in_vec):\n",
        "    out_vec = []\n",
        "    for t in in_vec:\n",
        "        t_stem = stemmer.stem(t)\n",
        "        if len(t_stem) > 2:\n",
        "            out_vec.append(t_stem)       \n",
        "    return out_vec"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S96ADZI_qtS"
      },
      "source": [
        "# Create user defined function for stemming with return type Array<String>\n",
        "from pyspark.sql.types import *\n",
        "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud-0ePu5_qgj"
      },
      "source": [
        "# Create new df with vectors containing the stemmed tokens \n",
        "vector_stemmed_df = (\n",
        "    vector_no_stopw_df\n",
        "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
        "        .select(\"vector_stemmed\")\n",
        "  )\n",
        "\n",
        "# Rename df and column for clarity\n",
        "production_df = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"bow\"))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXRj2dqcADbK",
        "outputId": "aaeeb79c-a630-42ed-91e1-a5ab63e0f725"
      },
      "source": [
        "# Display\n",
        "production_df.printSchema()\n",
        "production_df.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- bow: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+------------+\n",
            "|         bow|\n",
            "+------------+\n",
            "|          []|\n",
            "|      [post]|\n",
            "|          []|\n",
            "|      [part]|\n",
            "|          []|\n",
            "| [polyarchi]|\n",
            "|          []|\n",
            "|  [independ]|\n",
            "|      [blog]|\n",
            "|    [produc]|\n",
            "|          []|\n",
            "|          []|\n",
            "|     [polit]|\n",
            "|    [reform]|\n",
            "|   [program]|\n",
            "|          []|\n",
            "|       [new]|\n",
            "|   [america]|\n",
            "|          []|\n",
            "|[washington]|\n",
            "+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Fp3huM64JB"
      },
      "source": [
        "token_groups = production_df.groupby(col(\"bow\"))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9r4nvrN7Ejf",
        "outputId": "2a120cc0-2cce-4b2d-e3c8-27e1cf9d0daa"
      },
      "source": [
        "token_groups"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.group.GroupedData at 0x7fdc06745110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p9oLOlU8EUS"
      },
      "source": [
        "token_count =token_groups.count()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtHAYEXnkJ1X"
      },
      "source": [
        "BOW_vector=token_count.orderBy(\"count\",ascending=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvGH91Mcc5jt"
      },
      "source": [
        "BOW=BOW_vector.show(50)  ##create Bag Of Word"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}